import json
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import re
from konlpy.tag import Okt  # More widely available than Mecab
from rank_bm25 import BM25Okapi

# íŒŒì¼ ë¡œë“œ
filepath = r"/content/drive/MyDrive/[ì•„ì´ë¸Œë¦­ìŠ¤/ë°ëª¨_1293_í‘œë°í…ìŠ¤íŠ¸ê²°ê³¼ì¶”ì¶œ_250313.json"
with open(filepath, 'r') as f:
    data = json.load(f)

# ================= 1. í–¥ìƒëœ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© =================
# KoSBERT ëª¨ë¸ ì‚¬ìš© (ì˜ë¯¸ì  ìœ ì‚¬ì„±ì— íŠ¹í™”ëœ ëª¨ë¸)
model = SentenceTransformer('jhgan/ko-sbert-nli')


# ================= 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ =================
def preprocess_text(text):
    if not isinstance(text, str):
        return ""

    # ê¸°ë³¸ ì „ì²˜ë¦¬
    text = re.sub(r'\s+', ' ', text)  # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = text.strip()

    # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬ (ë‹¨, ì˜ë¯¸ìˆëŠ” íŠ¹ìˆ˜ë¬¸ìëŠ” ìœ ì§€)
    text = re.sub(r'[^\w\s\.\,\?\!\(\)\[\]\:\;\-\_\']', ' ', text)

    return text


# ================= 3. í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ë„êµ¬ ì´ˆê¸°í™” =================
okt = Okt()


def extract_keywords(text):
    if not isinstance(text, str):
        return ""

    # ëª…ì‚¬ ì¶”ì¶œ
    nouns = okt.nouns(text)

    # ì§§ì€ ëª…ì‚¬ í•„í„°ë§ (í•œê¸€ì ëª…ì‚¬ ì œê±°)
    filtered_nouns = [noun for noun in nouns if len(noun) > 1]

    # ëª…ì‚¬ê°€ ì—†ëŠ” ê²½ìš°, í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ì—ì„œ ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ íƒœê·¸ë¥¼ ê°€ì§„ ë‹¨ì–´ ì¶”ì¶œ
    if not filtered_nouns:
        pos_tags = okt.pos(text)
        filtered_nouns = [word for word, pos in pos_tags if pos in ['Noun', 'Verb', 'Adjective'] and len(word) > 1]

    return ' '.join(filtered_nouns)


# ================= 4. ì¿¼ë¦¬ í™•ì¥ í•¨ìˆ˜ =================
def expand_query(query_text):
    # ì›ë³¸ ì¿¼ë¦¬ ì „ì²˜ë¦¬
    processed_query = preprocess_text(query_text)

    # í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = extract_keywords(processed_query)

    # í™•ì¥ ì¿¼ë¦¬ ìƒì„±
    expanded_queries = [
        processed_query,  # ì›ë³¸ ì¿¼ë¦¬
        keywords,  # ì¶”ì¶œëœ í‚¤ì›Œë“œ
        processed_query + " " + keywords,  # ì›ë³¸ + í‚¤ì›Œë“œ
    ]

    # ì¤‘ë³µ ì œê±° ë° ë¹ˆ ë¬¸ìì—´ ì œê±°
    expanded_queries = [q for q in expanded_queries if q.strip()]
    expanded_queries = list(set(expanded_queries))

    # ì¿¼ë¦¬ê°€ ì§ˆë¬¸ í˜•íƒœì¸ ê²½ìš° ì§ˆë¬¸ ë§ˆì»¤ ì œê±°í•œ ë²„ì „ ì¶”ê°€
    if "?" in query_text or "ìˆë‚˜ìš”" in query_text or "ë¬´ì—‡" in query_text or "ì–´ë–¤" in query_text:
        # ì§ˆë¬¸í˜• í‘œí˜„ ì œê±°
        simple_query = re.sub(r'\?|ìˆë‚˜ìš”|ë¬´ì—‡ì¸ê°€ìš”|ì–´ë–¤ê°€ìš”|ì–´ë–¤ ê²ƒ|ë¬´ì—‡|ì–´ë–¤', '', processed_query).strip()
        if simple_query and simple_query not in expanded_queries:
            expanded_queries.append(simple_query)

    return expanded_queries


# ================= 5. í–¥ìƒëœ ì„ë² ë”© ìƒì„± í•¨ìˆ˜ =================
def get_enhanced_embedding(text):
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    processed_text = preprocess_text(text)

    if not processed_text:
        # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” 0ë²¡í„° ë°˜í™˜
        return np.zeros(768)

    # í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = extract_keywords(processed_text)

    # ì„ë² ë”© ìƒì„±
    text_embedding = model.encode(processed_text)

    # í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš° í‚¤ì›Œë“œ ì„ë² ë”© ìƒì„± ë° ê²°í•©
    if keywords:
        keyword_embedding = model.encode(keywords)
        # ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ì„ë² ë”© (ì›ë³¸:0.7, í‚¤ì›Œë“œ:0.3)
        combined_embedding = 0.7 * text_embedding + 0.3 * keyword_embedding
    else:
        combined_embedding = text_embedding

    # ì •ê·œí™”
    norm = np.linalg.norm(combined_embedding)
    if norm > 0:
        normalized_embedding = combined_embedding / norm
    else:
        normalized_embedding = combined_embedding

    return normalized_embedding


# ================= 6. BM25 ëª¨ë¸ ì´ˆê¸°í™” =================
# ë¬¸ì„œ í† í°í™”
tokenized_documents = []
for item in data:
    if 'content1' in item and isinstance(item['content1'], str):
        # ê° ë¬¸ì„œë¥¼ ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ í† í°í™”
        tokens = preprocess_text(item['content1']).split()
        tokenized_documents.append(tokens)
    else:
        tokenized_documents.append([])

# BM25 ëª¨ë¸ ìƒì„±
bm25 = BM25Okapi(tokenized_documents)

# ================= 7. ë¬¸ì„œ ì„ë² ë”© ìƒì„± =================
print("ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘...")
for i in range(len(data)):
    if 'content1' in data[i] and isinstance(data[i]['content1'], str):
        data[i]['enhanced_embedding'] = get_enhanced_embedding(data[i]['content1'])
    else:
        data[i]['enhanced_embedding'] = np.zeros(768)  # ì½˜í…ì¸ ê°€ ì—†ëŠ” ê²½ìš° 0ë²¡í„°


# ================= 8. ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ =================
def calculate_similarity_scores(query_text, data_items):
    # 1. ì¿¼ë¦¬ ì „ì²˜ë¦¬ ë° í™•ì¥
    expanded_queries = expand_query(query_text)
    print(f"í™•ì¥ëœ ì¿¼ë¦¬: {expanded_queries}")

    # 2. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embeddings = [get_enhanced_embedding(query) for query in expanded_queries]

    # 3. BM25 í† í°í™”
    tokenized_queries = [preprocess_text(query).split() for query in expanded_queries]

    # 4. ê° ë¬¸ì„œë§ˆë‹¤ ì ìˆ˜ ê³„ì‚°
    for i, item in enumerate(data_items):
        content_emb = item['enhanced_embedding']

        # 4.1 ì˜ë¯¸ì  ìœ ì‚¬ë„ - ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœëŒ€ê°’
        max_cosine_sim = max([
            cosine_similarity([content_emb], [q_emb])[0][0]
            for q_emb in query_embeddings
        ])

        # 4.2 ì–´íœ˜ì  ìœ ì‚¬ë„ - BM25 ì ìˆ˜ ìµœëŒ€ê°’ (ì •ê·œí™”)
        bm25_scores = [bm25.get_scores(tokenized_query)[i] for tokenized_query in tokenized_queries]
        max_bm25_score = max(bm25_scores) if bm25_scores else 0

        # ìµœëŒ€ BM25 ì ìˆ˜ ì°¾ê¸°
        all_bm25_scores = []
        for tokenized_query in tokenized_queries:
            scores = bm25.get_scores(tokenized_query)
            all_bm25_scores.extend(scores)

        max_possible_bm25 = max(all_bm25_scores) if all_bm25_scores else 1
        normalized_bm25 = max_bm25_score / max_possible_bm25 if max_possible_bm25 > 0 else 0

        # 4.3 ë‹¨ì–´ ë¹ˆë„ ì ìˆ˜
        content = item.get('content1', '').lower()
        query_keywords = ' '.join([extract_keywords(q) for q in expanded_queries]).lower()
        query_keywords = ' '.join(list(set(query_keywords.split())))  # ì¤‘ë³µ ì œê±°

        # ì¿¼ë¦¬ í‚¤ì›Œë“œë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°
        keyword_weight = 0
        for keyword in query_keywords.split():
            if len(keyword) > 1 and keyword in content:
                # í‚¤ì›Œë“œ ê¸¸ì´ì— ë¹„ë¡€í•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬ (ê¸´ í‚¤ì›Œë“œê°€ ë” ì¤‘ìš”)
                keyword_weight += len(keyword) * 0.01

        # 4.4 ìµœì¢… ì ìˆ˜ ê³„ì‚° (ì˜ë¯¸:65%, ì–´íœ˜:25%, í‚¤ì›Œë“œ:10%)
        item['semantic_score'] = max_cosine_sim
        item['lexical_score'] = normalized_bm25
        item['keyword_score'] = min(keyword_weight, 0.2)  # ìµœëŒ€ 0.2ë¡œ ì œí•œ

        # ìµœì¢… ì ìˆ˜ëŠ” ì„¸ ìš”ì†Œì˜ ê°€ì¤‘í•©
        item['final_score'] = (0.65 * max_cosine_sim) + (0.25 * normalized_bm25) + (0.1 * item['keyword_score'])

    return data_items


# ================= 9. ê²€ìƒ‰ í•¨ìˆ˜ =================
def semantic_search(query_text, data_items, top_k=15):
    # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
    scored_items = calculate_similarity_scores(query_text, data_items)

    # ì ìˆ˜ìˆœ ì •ë ¬ ë° ìƒìœ„ Kê°œ ì¶”ì¶œ
    k = min(top_k, len(scored_items))
    top_k_items = sorted(scored_items, key=lambda x: x['final_score'], reverse=True)[:k]

    return top_k_items


# ================= 10. ê²°ê³¼ ì¶œë ¥ í•¨ìˆ˜ =================
def print_search_results(results, query_text, target_content=None):
    print(f"\nâœ… ì¿¼ë¦¬: \"{query_text}\"ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼:")

    for i, item in enumerate(results):
        print(f"\nğŸ”¹ {i + 1}ìœ„ ì²­í¬ (ìµœì¢… ì ìˆ˜: {item['final_score']:.4f}):")
        print(f"  - ì˜ë¯¸ì  ìœ ì‚¬ë„: {item['semantic_score']:.4f}")
        print(f"  - ì–´íœ˜ì  ìœ ì‚¬ë„: {item['lexical_score']:.4f}")
        print(f"  - í‚¤ì›Œë“œ ì ìˆ˜: {item['keyword_score']:.4f}")
        print(f"  - ë‚´ìš©: {item['content1']}")

    # ëª©í‘œ ì²­í¬ ê²€ìƒ‰ (ë””ë²„ê¹…ìš©)
    if target_content:
        all_ranked = sorted(data, key=lambda x: x['final_score'], reverse=True)
        for i, item in enumerate(all_ranked):
            if target_content in item.get('content1', ''):
                print(f"\nâ­ ëª©í‘œ ì²­í¬ \"{target_content}...\"ëŠ” {i + 1}ìœ„ì— ë­í¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                print(f"  - ìµœì¢… ì ìˆ˜: {item['final_score']:.4f}")
                print(f"  - ì˜ë¯¸ì  ìœ ì‚¬ë„: {item['semantic_score']:.4f}")
                print(f"  - ì–´íœ˜ì  ìœ ì‚¬ë„: {item['lexical_score']:.4f}")
                print(f"  - í‚¤ì›Œë“œ ì ìˆ˜: {item['keyword_score']:.4f}")
                print(f"  - ì „ì²´ ë‚´ìš©: {item['content1']}")
                break


# ì™œ ë¼..?

# ================= 11. ì‹¤í–‰ ì˜ˆì‹œ =================
# ìƒ˜í”Œ ì¿¼ë¦¬ ì‹¤í–‰
query_text = "OLEDì˜ ì¥ì ì€ ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”?"  # 3ìœ„
# query_text = "í”Œë¼ìŠ¤í‹± LCD(plastic LCD)ê°€ ê¸°ì¡´ì˜ ìœ ë¦¬ì— ë¹„í•´ ê°–ê³  ìˆëŠ” ì´ìµì€?"#1ìœ„
# query_text = "FMC ì‚¬ì—… í™œì„±í™”ë¥¼ ìœ„í•´ì„œ ì •ì±…ì°¨ì›ì—ì„œëŠ” ì–´ë–¤ ë°©í–¥ì´ ìˆëŠ”ê°€?"#1ìœ„
# query_text = "ê¸°ìˆ ì  ìœµí–¡ì˜ ê°€ì¥ ê¸°ì´ˆì ì¸ ë‹¨ê³„ëŠ”?"#1ìœ„
# query_text = "ë¯¸êµ­ì˜ UDC ì™€ ì¼ë³¸ì˜ Pioneer ì‚¬ê°€ êµ¬í˜„í•œ ê²ƒì€?"#1ìœ„
# query_text = "ì´ìš©ì ê´€ì ì˜ ì´ìŠˆëŠ” ì„œë¹„ìŠ¤ ì´ìš© ìš•êµ¬ëŠ”?"#1ìœ„

results = semantic_search(query_text, data, top_k=3)
print_search_results(results, query_text)


# ================= 12. í•¨ìˆ˜í™”í•˜ì—¬ ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ =================
def process_query(query_text, target_content=None):
    """
    ì£¼ì–´ì§„ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜

    Args:
        query_text (str): ê²€ìƒ‰ ì¿¼ë¦¬
        target_content (str, optional): ëª©í‘œ ì²­í¬ì˜ ì¼ë¶€ ë‚´ìš© (ë””ë²„ê¹…ìš©)

    Returns:
        list: ìƒìœ„ ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡
    """
    results = semantic_search(query_text, data, top_k=15)
    print_search_results(results, query_text, target_content)
    return results

# í•¨ìˆ˜ ì‚¬ìš© ì˜ˆì‹œ:
# process_query("OLEDì˜ ì¥ì ì€ ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”?", "OLEDëŠ” ìì²´ë°œê´‘ ë””ìŠ¤í”Œë ˆì´ë¡œ")